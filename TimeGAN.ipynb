{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGAN(keras.Model):\n",
    "    def __init__(self, seq_len, n_features, hidden_dim):\n",
    "        super(TimeGAN, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Generator\n",
    "        self.embedder = self._build_network(name=\"embedder\")\n",
    "        self.recovery = self._build_network(name=\"recovery\")\n",
    "        self.generator = self._build_network(name=\"generator\")\n",
    "\n",
    "        # Discriminator\n",
    "        self.discriminator = self._build_discriminator()\n",
    "\n",
    "        # Supervisor\n",
    "        self.supervisor = self._build_network(name=\"supervisor\")\n",
    "\n",
    "    def _build_network(self, name):\n",
    "        return keras.Sequential([\n",
    "            keras.layers.GRU(units=self.hidden_dim, return_sequences=True),\n",
    "            keras.layers.GRU(units=self.hidden_dim, return_sequences=True),\n",
    "            keras.layers.TimeDistributed(keras.layers.Dense(units=self.n_features))\n",
    "        ], name=name)\n",
    "\n",
    "    def _build_discriminator(self):\n",
    "        return keras.Sequential([\n",
    "            keras.layers.GRU(units=self.hidden_dim, return_sequences=True),\n",
    "            keras.layers.GRU(units=self.hidden_dim, return_sequences=True),\n",
    "            keras.layers.TimeDistributed(keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "        ], name=\"discriminator\")\n",
    "\n",
    "    def embed(self, x):\n",
    "        return self.embedder(x)\n",
    "\n",
    "    def supervise(self, h):\n",
    "        return self.supervisor(h)\n",
    "\n",
    "    def generate(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def reconstruct(self, h):\n",
    "        return self.recovery(h)\n",
    "\n",
    "    def discriminate(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, real_data):\n",
    "        # Embedding\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        random_noise = tf.random.normal(shape=(batch_size, self.seq_len, self.hidden_dim))\n",
    "        hidden = self.embed(real_data)\n",
    "        \n",
    "        with tf.GradientTape() as tape_g, tf.GradientTape() as tape_e, tf.GradientTape() as tape_d:\n",
    "            # Generator\n",
    "            fake_data = self.generate(random_noise)\n",
    "            \n",
    "            # Supervisor\n",
    "            generated_hidden = self.embed(fake_data)\n",
    "            supervised_fake = self.supervise(hidden)\n",
    "\n",
    "            # Discriminator\n",
    "            real_output = self.discriminate(real_data)\n",
    "            fake_output = self.discriminate(fake_data)\n",
    "\n",
    "            # Losses\n",
    "            # Reconstruction loss\n",
    "            e_loss_t0 = tf.reduce_mean((real_data - self.reconstruct(hidden))**2)\n",
    "            e_loss_0 = 10 * tf.sqrt(e_loss_t0)\n",
    "            \n",
    "            # Supervised loss\n",
    "            g_loss_s = tf.reduce_mean((hidden[:, 1:, :] - supervised_fake[:, :-1, :])**2)\n",
    "\n",
    "            # Unsupervised loss\n",
    "            g_loss_u = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output))\n",
    "            \n",
    "            # Discriminator loss\n",
    "            d_loss_real = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output))\n",
    "            d_loss_fake = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output))\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            # Generator loss\n",
    "            g_loss = g_loss_u + 100 * g_loss_s\n",
    "\n",
    "            # Embedding network loss\n",
    "            e_loss = e_loss_0 + 0.1 * g_loss_s\n",
    "\n",
    "        # Compute gradients\n",
    "        e_gradients = tape_e.gradient(e_loss, self.embedder.trainable_variables + self.recovery.trainable_variables)\n",
    "        g_gradients = tape_g.gradient(g_loss, self.generator.trainable_variables + self.supervisor.trainable_variables)\n",
    "        d_gradients = tape_d.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        # Apply gradients\n",
    "        self.optimizer.apply_gradients(zip(e_gradients, self.embedder.trainable_variables + self.recovery.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables + self.supervisor.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "\n",
    "        return {\"e_loss\": e_loss, \"g_loss\": g_loss, \"d_loss\": d_loss}\n",
    "\n",
    "    def generate_samples(self, n_samples):\n",
    "        random_noise = tf.random.normal(shape=(n_samples, self.seq_len, self.hidden_dim))\n",
    "        return self.generate(random_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, seq_len, train_split=0.8):\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    for i in range(len(scaled_data) - seq_len + 1):\n",
    "        sequences.append(scaled_data[i:i+seq_len])\n",
    "    sequences = np.array(sequences)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_data, test_data = train_test_split(sequences, train_size=train_split, shuffle=False)\n",
    "    \n",
    "    return train_data, test_data, scaler\n",
    "\n",
    "# 2. Initialize and train the model\n",
    "def train_timegan(train_data, epochs=100, batch_size=32):\n",
    "    seq_len, n_features = train_data.shape[1:]\n",
    "    hidden_dim = 24  # You can adjust this\n",
    "\n",
    "    model = TimeGAN(seq_len, n_features, hidden_dim)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.fit(train_data, epochs=1, batch_size=batch_size)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 3. Generate synthetic samples\n",
    "def generate_synthetic_samples(model, n_samples):\n",
    "    return model.generate_samples(n_samples)\n",
    "\n",
    "# 4. Combine original and synthetic data\n",
    "def augment_dataset(original_data, synthetic_data):\n",
    "    return np.concatenate([original_data, synthetic_data], axis=0)\n",
    "\n",
    "# 5. Validate the augmented dataset\n",
    "def validate_augmented_data(original_data, augmented_data):\n",
    "    # Compare basic statistics\n",
    "    print(\"Original data shape:\", original_data.shape)\n",
    "    print(\"Augmented data shape:\", augmented_data.shape)\n",
    "    \n",
    "    print(\"\\nOriginal data statistics:\")\n",
    "    print(np.mean(original_data, axis=(0,1)))\n",
    "    print(np.std(original_data, axis=(0,1)))\n",
    "    \n",
    "    print(\"\\nAugmented data statistics:\")\n",
    "    print(np.mean(augmented_data, axis=(0,1)))\n",
    "    print(np.std(augmented_data, axis=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 24  # Adjust based on your needs\n",
    "train_data, test_data, scaler = prepare_data(data, seq_len)\n",
    "\n",
    "# 2. Train the model\n",
    "model = train_timegan(train_data, epochs=100)\n",
    "\n",
    "# 3. Generate synthetic samples\n",
    "n_synthetic = len(train_data)  # Generate as many synthetic samples as original\n",
    "synthetic_data = generate_synthetic_samples(model, n_synthetic)\n",
    "\n",
    "# 4. Combine original and synthetic data\n",
    "augmented_data = augment_dataset(train_data, synthetic_data)\n",
    "\n",
    "# 5. Validate the augmented dataset\n",
    "validate_augmented_data(train_data, augmented_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
