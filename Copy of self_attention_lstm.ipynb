{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126040,"status":"ok","timestamp":1725519864856,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"0ppQeUvGwaAR","outputId":"b4e222c6-f79a-415f-e3cc-9a536ba09403"},"outputs":[],"source":["# !pip install tsgm"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":30086,"status":"ok","timestamp":1725519894932,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"aK-6tQbMp1sg"},"outputs":[],"source":["import tensorflow as tf\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, LayerNormalization, Dense, Attention, MultiHeadAttention, Lambda\n","from tensorflow.keras.models import Model, Sequential\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from tensorflow.keras import backend as K\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","from sklearn.metrics import ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# import tsgm"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13555,"status":"ok","timestamp":1725519908484,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"NHsvN4F9qWj4","outputId":"8f20d8be-5003-4b06-a1c7-6bd7d3c2cb8b"},"outputs":[],"source":["# !gdown --fuzzy https://drive.google.com/file/d/1F9uinZY-eG4x9dNsUOOtAAZMYq6p945U/view?usp=drive_link\n","# !unzip -qq \"ASL-Sensor-Dataglove-Dataset.zip\" -d glove_data\n","# !echo \"Unzip successfully\""]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1725519908485,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"xX-l2zuop1sh"},"outputs":[],"source":["class TimeSeriesDataset:\n","    def __init__(self, root_dir, feature_names=[]):\n","        self.data = self.load_data(root_dir, feature_names)\n","        self.merge_flex_sensors()\n","\n","    def load_data(self, root_dir, feature_names):\n","        data = []\n","\n","        for individual_dir in sorted(os.listdir(root_dir)):\n","            individual_path = os.path.join(root_dir, individual_dir)\n","            for class_dir in sorted(os.listdir(individual_path)):\n","                class_path = os.path.join(individual_path, class_dir)\n","                if os.path.isdir(class_path):\n","                    for file in glob.glob(os.path.join(class_path, \"*.csv\")):\n","                        df = pd.read_csv(file, usecols=feature_names)\n","                        class_name = os.path.splitext(os.path.basename(file))[0]\n","                        df[\"class\"] = class_name\n","                        data.append(df)\n","\n","\n","        # Concatenate all data frames into a single data frame\n","        data = pd.concat(data, ignore_index=True)\n","        return data\n","    \n","    def merge_flex_sensors(self):\n","        if 'flex_4' in self.data.columns and 'flex_5' in self.data.columns:\n","            # Define a small epsilon value to avoid division by zero\n","            epsilon = 1e-10\n","\n","            # Convert flex sensor values to conductance (1/R), handling zero values\n","            conductance_4 = 1 / (self.data['flex_4'] + epsilon)\n","            conductance_5 = 1 / (self.data['flex_5'] + epsilon)\n","            \n","            # Sum the conductances\n","            total_conductance = conductance_4 + conductance_5\n","            \n","            # Convert back to resistance, handling very large values\n","            self.data['flex_4'] = np.where(\n","                total_conductance > epsilon,\n","                1 / total_conductance,\n","                np.finfo(float).max  # Use maximum float value for near-zero conductance\n","            )\n","            \n","            # Drop original columns\n","            self.data = self.data.drop(columns=['flex_5'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":16265,"status":"ok","timestamp":1725519924739,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"_1mCno1dp1sh"},"outputs":[],"source":["root_dir = \"glove_data/\"\n","feature_names = [\n","    \"flex_1\", \"flex_2\", \"flex_3\", \"flex_4\",\n","    \"GYRx\", \"GYRy\", \"GYRz\"\n","]\n","\n","dataset = TimeSeriesDataset(root_dir, feature_names).data\n","# dataset = dataset.sort_values(by=[\"class\"])\n","\n","# filter_classes = [\"deaf\", \"fine\", \"good\", \"goodbye\", \"hello\"]\n","# dataset = dataset[dataset[\"class\"].isin(filter_classes)]\n","\n","x_data, y_data = dataset.iloc[:, :-1].values, dataset.iloc[:, -1].values\n","\n","scaler = StandardScaler()\n","x_data = scaler.fit_transform(x_data)\n","\n","label_encoder = LabelEncoder()\n","y_data = label_encoder.fit_transform(y_data)\n","\n","timesteps = 150\n","n_features = 7\n","num_classes = len(np.unique(y_data))\n","\n","num_samples = len(y_data) // timesteps\n","\n","x_data = x_data[:num_samples * timesteps].reshape((num_samples, timesteps, n_features))\n","y_data = y_data[:num_samples * timesteps:timesteps]\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(10000, 240, 7)\n"]}],"source":["from scipy import interpolate\n","\n","\n","# Define original and new timesteps\n","old_timesteps = np.arange(150)\n","new_timesteps = np.linspace(0, 149, 240)  # New time steps from 0 to 149, but 240 steps in between\n","\n","# Initialize an empty array for the upsampled data\n","upsampled_data = np.zeros((x_data.shape[0], 240, x_data.shape[2]))\n","\n","# Interpolate along the time dimension for each sample and each feature\n","for i in range(x_data.shape[0]):  # Iterate over each sample\n","    for j in range(x_data.shape[2]):  # Iterate over each feature\n","        f = interpolate.interp1d(old_timesteps, x_data[i, :, j], kind='linear')\n","        upsampled_data[i, :, j] = f(new_timesteps)\n","\n","print(upsampled_data.shape)  # Should print (10000, 240, 7)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["x_data = upsampled_data\n","timesteps = 240"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1725519924739,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"ZLd4VjO5K-7p"},"outputs":[],"source":["# aug_model = tsgm.models.augmentations.GaussianNoise()\n","# x_data_aug = aug_model.generate(x_data, n_samples=x_data.shape[0], variance=0.2)\n","\n","# x_data = np.concatenate((x_data, x_data_aug), axis=0)\n","# y_data = np.concatenate((y_data, y_data), axis=0)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1725519924740,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"yQozsMv1yJMP","outputId":"5da34a12-6f05-4e0f-b429-bd5d55433cdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10000, 240, 7)\n","(10000,)\n"]}],"source":["print(x_data.shape)\n","print(y_data.shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1725519924740,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"ij1_LtWpK7kE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1725519924740,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"D1JBuYi5p1si"},"outputs":[],"source":["def positional_encoding(length, depth):\n","    depth = int(depth)\n","    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","\n","    angle_rates = 1 / (10000**depths)                # (1, depth)\n","    angle_rads = positions * angle_rates             # (pos, depth)\n","\n","    pos_encoding = np.concatenate(\n","        [np.sin(angle_rads), np.cos(angle_rads)],\n","        axis=-1)\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","class AddPositionalEncoding(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def build(self, input_shape):\n","        _, seq_len, d_model = input_shape\n","        self.pos_encoding = positional_encoding(seq_len, d_model)\n","\n","    def call(self, inputs):\n","        # Ensure positional encoding has the same shape as the input\n","        return inputs + self.pos_encoding[:tf.shape(inputs)[1], :tf.shape(inputs)[2]]\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        return config\n","\n","def create_model(timesteps, n_features, num_classes):\n","    inputs = Input(shape=(timesteps, n_features))\n","\n","    x = Conv1D(filters=64, kernel_size=5, activation='relu')(inputs)\n","    x = MaxPooling1D(pool_size=2)(x)\n","\n","    x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n","    x = MaxPooling1D(pool_size=2)(x)\n","\n","    x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n","    x = MaxPooling1D(pool_size=2)(x)\n","\n","    x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n","    x = MaxPooling1D(pool_size=2)(x)\n","\n","    x = LSTM(units=128, return_sequences=True)(x)\n","    x = LSTM(units=128, return_sequences=True)(x)\n","\n","    x = AddPositionalEncoding()(x)\n","\n","    # MultiHeadAttention layer\n","    attn_output = MultiHeadAttention(num_heads=4, key_dim=128)(x, x, x)\n","    x = LayerNormalization()(attn_output + x)\n","\n","    x = Dense(units=128, activation='relu')(x)\n","\n","    # Global Attention layer\n","    attn = Attention()([x, x])\n","    x = LayerNormalization()(attn + x)\n","\n","    # Global average pooling to reduce sequence dimension\n","    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n","\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51545,"status":"ok","timestamp":1725519976273,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"IWIToYW-p1si","outputId":"a2b25791-a65a-42ba-ff53-8b19d9cd9c03"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_2 (InputLayer)        [(None, 240, 7)]             0         []                            \n","                                                                                                  \n"," conv1d_4 (Conv1D)           (None, 236, 64)              2304      ['input_2[0][0]']             \n","                                                                                                  \n"," max_pooling1d_4 (MaxPoolin  (None, 118, 64)              0         ['conv1d_4[0][0]']            \n"," g1D)                                                                                             \n","                                                                                                  \n"," conv1d_5 (Conv1D)           (None, 114, 64)              20544     ['max_pooling1d_4[0][0]']     \n","                                                                                                  \n"," max_pooling1d_5 (MaxPoolin  (None, 57, 64)               0         ['conv1d_5[0][0]']            \n"," g1D)                                                                                             \n","                                                                                                  \n"," conv1d_6 (Conv1D)           (None, 53, 64)               20544     ['max_pooling1d_5[0][0]']     \n","                                                                                                  \n"," max_pooling1d_6 (MaxPoolin  (None, 26, 64)               0         ['conv1d_6[0][0]']            \n"," g1D)                                                                                             \n","                                                                                                  \n"," conv1d_7 (Conv1D)           (None, 22, 64)               20544     ['max_pooling1d_6[0][0]']     \n","                                                                                                  \n"," max_pooling1d_7 (MaxPoolin  (None, 11, 64)               0         ['conv1d_7[0][0]']            \n"," g1D)                                                                                             \n","                                                                                                  \n"," lstm_2 (LSTM)               (None, 11, 128)              98816     ['max_pooling1d_7[0][0]']     \n","                                                                                                  \n"," lstm_3 (LSTM)               (None, 11, 128)              131584    ['lstm_2[0][0]']              \n","                                                                                                  \n"," add_positional_encoding_1   (None, 11, 128)              0         ['lstm_3[0][0]']              \n"," (AddPositionalEncoding)                                                                          \n","                                                                                                  \n"," multi_head_attention_1 (Mu  (None, 11, 128)              263808    ['add_positional_encoding_1[0]\n"," ltiHeadAttention)                                                  [0]',                         \n","                                                                     'add_positional_encoding_1[0]\n","                                                                    [0]',                         \n","                                                                     'add_positional_encoding_1[0]\n","                                                                    [0]']                         \n","                                                                                                  \n"," tf.__operators__.add_2 (TF  (None, 11, 128)              0         ['multi_head_attention_1[0][0]\n"," OpLambda)                                                          ',                            \n","                                                                     'add_positional_encoding_1[0]\n","                                                                    [0]']                         \n","                                                                                                  \n"," layer_normalization_2 (Lay  (None, 11, 128)              256       ['tf.__operators__.add_2[0][0]\n"," erNormalization)                                                   ']                            \n","                                                                                                  \n"," dense_2 (Dense)             (None, 11, 128)              16512     ['layer_normalization_2[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," attention_1 (Attention)     (None, 11, 128)              0         ['dense_2[0][0]',             \n","                                                                     'dense_2[0][0]']             \n","                                                                                                  \n"," tf.__operators__.add_3 (TF  (None, 11, 128)              0         ['attention_1[0][0]',         \n"," OpLambda)                                                           'dense_2[0][0]']             \n","                                                                                                  \n"," layer_normalization_3 (Lay  (None, 11, 128)              256       ['tf.__operators__.add_3[0][0]\n"," erNormalization)                                                   ']                            \n","                                                                                                  \n"," global_average_pooling1d_1  (None, 128)                  0         ['layer_normalization_3[0][0]'\n","  (GlobalAveragePooling1D)                                          ]                             \n","                                                                                                  \n"," dense_3 (Dense)             (None, 40)                   5160      ['global_average_pooling1d_1[0\n","                                                                    ][0]']                        \n","                                                                                                  \n","==================================================================================================\n","Total params: 580328 (2.21 MB)\n","Trainable params: 580328 (2.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","Epoch 1/30\n","125/125 [==============================] - 21s 120ms/step - loss: 1.7121 - accuracy: 0.4560 - val_loss: 1.1236 - val_accuracy: 0.5700\n","Epoch 2/30\n","125/125 [==============================] - 14s 109ms/step - loss: 0.8891 - accuracy: 0.6365 - val_loss: 1.0800 - val_accuracy: 0.5690\n","Epoch 3/30\n","125/125 [==============================] - 13s 107ms/step - loss: 0.7909 - accuracy: 0.6762 - val_loss: 0.9378 - val_accuracy: 0.6205\n","Epoch 4/30\n","125/125 [==============================] - 13s 105ms/step - loss: 0.7283 - accuracy: 0.6964 - val_loss: 0.9106 - val_accuracy: 0.6395\n","Epoch 5/30\n","125/125 [==============================] - 12s 97ms/step - loss: 0.6949 - accuracy: 0.7046 - val_loss: 0.9836 - val_accuracy: 0.6290\n","Epoch 6/30\n","125/125 [==============================] - 12s 96ms/step - loss: 0.6722 - accuracy: 0.7234 - val_loss: 1.1134 - val_accuracy: 0.6100\n","Epoch 7/30\n","125/125 [==============================] - 14s 113ms/step - loss: 0.6390 - accuracy: 0.7304 - val_loss: 1.0244 - val_accuracy: 0.6215\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x196b68eb590>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["model = create_model(timesteps, n_features, num_classes)\n","model.summary()\n","\n","# checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model_weights.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(x_data, y_data, epochs=30, batch_size=64, validation_split = 0.2, callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["if not os.path.exists(\"model\"):\n","    os.makedirs(\"model\")\n","model.save(\"model/SADeepConvLSTM_TransferLearning.h5\")"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2202,"status":"ok","timestamp":1725519978462,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"CIlZKH1arDQs","outputId":"6d9aadf7-fc43-4f59-cd7d-1052162dbdfb"},"outputs":[],"source":["# y_pred = model.predict(x_test)\n","# y_pred = np.argmax(y_pred, axis=1)\n","# accuracy = accuracy_score(y_test, y_pred)\n","# print(\"Accuracy:\", accuracy)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"elapsed":5291,"status":"ok","timestamp":1725519983751,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"xAqjjKJfsAxl","outputId":"e7f2ec60-fdfb-4bdc-b785-80fd5d133c4d"},"outputs":[],"source":["# cm = confusion_matrix(y_test, y_pred)\n","# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_encoder.classes_))\n","# disp.plot(cmap=plt.cm.Blues)\n","# plt.show()"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1725519983751,"user":{"displayName":"Hòa Nguyễn","userId":"08524944918431446312"},"user_tz":-420},"id":"HXyJfjuBN6Rs","outputId":"c12355fa-e3e4-45e2-ec77-632d419801ef"},"outputs":[],"source":["# print(classification_report(y_test, y_pred))"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/orangethefish/ASL_Glove/blob/main/self_attention_lstm.ipynb","timestamp":1721873593286}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
