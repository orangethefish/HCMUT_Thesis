{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "dataset_path = 'datatrain_40/total'\n",
    "input_file = os.path.join(dataset_path, 'trainx.txt')\n",
    "output_path = os.path.join(dataset_path, \"augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_txt_data(data, file_path):\n",
    "    \"\"\"\n",
    "    Save data to a .txt file.\n",
    "    \n",
    "    :param data: numpy array of shape (n_samples, 1680)\n",
    "    :param file_path: str, path to save the .txt file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        np.savetxt(file_path, data, fmt='%.6f', delimiter=' ')\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving file {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_txt_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a .txt file.\n",
    "    \n",
    "    :param file_path: str, path to the .txt file\n",
    "    :return: numpy array of shape (n_samples, 1680)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in lines:\n",
    "        # Split the line by whitespace and convert to float\n",
    "        values = [float(val) for val in line.strip().split()]\n",
    "        if len(values) != 1680:\n",
    "            raise ValueError(f\"Expected 1680 values per line, but got {len(values)}\")\n",
    "        data.append(values)\n",
    "    \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jitter(data, noise_factor=0.05):\n",
    "    \"\"\"\n",
    "    Apply jittering to the input data.\n",
    "    \n",
    "    :param data: numpy array of shape (n_samples, 1680)\n",
    "    :param noise_factor: float, amount of noise to add (default: 0.05)\n",
    "    :return: jittered data with the same shape as input\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(loc=0, scale=noise_factor, size=data.shape)\n",
    "    jittered_data = data + noise * np.abs(data)\n",
    "    return jittered_data\n",
    "\n",
    "def scale(data, scaling_factor=0.1):\n",
    "    \"\"\"\n",
    "    Apply random scaling to the input data.\n",
    "    \n",
    "    :param data: numpy array of shape (n_samples, 1680)\n",
    "    :param scaling_factor: float, maximum scaling factor (default: 0.1)\n",
    "    :return: scaled data with the same shape as input\n",
    "    \"\"\"\n",
    "    factors = np.random.uniform(1 - scaling_factor, 1 + scaling_factor, size=(data.shape[0], 1))\n",
    "    scaled_data = data * factors\n",
    "    return scaled_data\n",
    "\n",
    "def shift(data, max_shift=10):\n",
    "    \"\"\"\n",
    "    Apply random time shift to the input data.\n",
    "    \n",
    "    :param data: numpy array of shape (n_samples, 1680)\n",
    "    :param max_shift: int, maximum number of time steps to shift (default: 10)\n",
    "    :return: time-shifted data with the same shape as input\n",
    "    \"\"\"\n",
    "    shifted_data = np.zeros_like(data)\n",
    "    for i in range(data.shape[0]):\n",
    "        shift = np.random.randint(-max_shift, max_shift + 1)\n",
    "        shifted_data[i] = np.roll(data[i], shift)\n",
    "    return shifted_data\n",
    "\n",
    "def magnitude_warp(data, sigma=0.2, knot=4):\n",
    "    \"\"\"\n",
    "    Apply magnitude warping to the input data.\n",
    "    \n",
    "    :param data: numpy array of shape (n_samples, 1680)\n",
    "    :param sigma: float, standard deviation of the warping (default: 0.2)\n",
    "    :param knot: int, number of knot points for warping (default: 4)\n",
    "    :return: magnitude-warped data with the same shape as input\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    \n",
    "    warped_data = np.zeros_like(data)\n",
    "    time_steps = 240  # Assuming 240 time steps per sensor\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        time_points = np.arange(time_steps)\n",
    "        knot_points = np.linspace(0, time_steps - 1, knot)\n",
    "        coeffs = np.random.normal(loc=1.0, scale=sigma, size=(7, knot))  # 7 sensors\n",
    "        \n",
    "        for j in range(7):  # For each sensor\n",
    "            spline = CubicSpline(knot_points, coeffs[j])\n",
    "            warps = spline(time_points)\n",
    "            warped_data[i, j*time_steps:(j+1)*time_steps] = data[i, j*time_steps:(j+1)*time_steps] * warps\n",
    "    \n",
    "    return warped_data\n",
    "\n",
    "def combine_augmentations(data, techniques=['jitter', 'scale', 'shift', 'warp'], **kwargs):\n",
    "    \"\"\"\n",
    "    Apply multiple augmentation techniques to the input data.\n",
    "    \n",
    "    :param data: numpy array of shape (n_samples, 1680)\n",
    "    :param techniques: list of strings, techniques to apply (default: all)\n",
    "    :param kwargs: additional arguments for each technique\n",
    "    :return: augmented data with the same shape as input\n",
    "    \"\"\"\n",
    "    augmented_data = data.copy()\n",
    "    \n",
    "    if 'jitter' in techniques:\n",
    "        augmented_data = jitter(augmented_data, kwargs.get('noise_factor', 0.05))\n",
    "    if 'scale' in techniques:\n",
    "        augmented_data = scale(augmented_data, kwargs.get('scaling_factor', 0.1))\n",
    "    if 'shift' in techniques:\n",
    "        augmented_data = shift(augmented_data, kwargs.get('max_shift', 10))\n",
    "    if 'warp' in techniques:\n",
    "        augmented_data = magnitude_warp(augmented_data, kwargs.get('sigma', 0.2), kwargs.get('knot', 4))\n",
    "    \n",
    "    return augmented_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_techniques = [jitter, scale, shift, magnitude_warp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the input data\n",
    "    original_data = load_txt_data(input_file)\n",
    "    \n",
    "    # Apply the base techniques\n",
    "    for technique in current_techniques:\n",
    "        data = technique(original_data)\n",
    "        file_name = f\"{technique.__name__}.txt\"\n",
    "        full_path = os.path.join(output_path, file_name)\n",
    "        save_txt_data(data, full_path)\n",
    "\n",
    "    # Select a pair of techniques to combine\n",
    "    for i in range(len(current_techniques)):\n",
    "        for j in range(i+1, len(current_techniques)):\n",
    "            technique1 = current_techniques[i]\n",
    "            technique2 = current_techniques[j]\n",
    "            data = technique1(original_data)\n",
    "            data = technique2(data)\n",
    "            file_name = f\"{technique1.__name__}_{technique2.__name__}.txt\"\n",
    "            full_path = os.path.join(output_path, file_name)\n",
    "            save_txt_data(data, full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
