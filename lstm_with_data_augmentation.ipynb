{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from numpy import array, dstack\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, LayerNormalization, Dense, Attention, MultiHeadAttention, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "# import pygame\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "#pygame.init()\n",
    "# engine = pyttsx3.init() # object creation\n",
    "SERIAL_PORT = '/dev/cu.DATN'\n",
    "# be sure to set this to the same rate used on the Arduino\n",
    "SERIAL_RATE = 38400\n",
    "test_counter = 240\n",
    "queue_size = 240\n",
    "verbose, epochs, batch_size = 1, 5, 300\n",
    "dataset_path = \"./datatrain_40/total/\"\n",
    "trainx_file = \"./datatrain_40/total/trainx.txt\"\n",
    "trainy_file = \"./datatrain_40/total/trainy.txt\"\n",
    "testx_file = \"./datatrain_40/total/testx.txt\"\n",
    "testy_file = \"./datatrain_40/total/testy.txt\"\n",
    "config_file = \"./datatrain_40/total/config.txt\"\n",
    "enlarged_dataset_path = \"./datatrain_40/total/augmentation/\"\n",
    "\n",
    "\n",
    "# screen_width = 800\n",
    "# screen_height = 600\n",
    "# screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "# pygame.display.set_caption(\"Real-time Display\")\n",
    "\n",
    "# Set up fonts\n",
    "#font = pygame.font.Font(None, 36)\n",
    "#WHITE = (255, 255, 255)\n",
    "\"\"\" RATE\"\"\"\n",
    "#rate = engine.getProperty('rate')   # getting details of current speaking rate\n",
    "# engine.setProperty('rate', 125)     # setting up new voice rate\n",
    "\"\"\"VOLUME\"\"\"\n",
    "#volume = engine.getProperty('volume')   #getting to know current volume level (min=0 and max=1)\n",
    "#print (volume)                          #printing current volume level\n",
    "# engine.setProperty('volume',1.0)    # setting up volume level  between 0 and 1\n",
    "\n",
    "\"\"\"VOICE\"\"\"\n",
    "#voices = engine.getProperty('voices')       #getting details of current voice\n",
    "#engine.setProperty('voice', voices[0].id)  #changing index, changes voices. o for male\n",
    "# engine.setProperty('voice', engine.getProperty('voices')[1].id)   #changing index, changes voices. 1 for female\n",
    "\n",
    "# def textToSpeech(text):\n",
    "#     engine.say(text)\n",
    "#     engine.runAndWait()\n",
    "#     engine.stop()\n",
    "    \n",
    "\n",
    "def readConfig():\n",
    "    with open('./datatrain_40/total/config.txt', 'r',encoding='utf-8') as file:\n",
    "        # Initialize an empty 2D array\n",
    "        config = []\n",
    "\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Split the line into individual words\n",
    "            words = line.strip()\n",
    "\n",
    "            # Append the words to the 2D array\n",
    "            config.append(words)\n",
    "        return config\n",
    "\n",
    "def readdata(trainx_file, trainy_file, testx_file, testy_file):\n",
    "    trainx_data =[]\n",
    "    trainy_data =[]\n",
    "    testx_data =[]\n",
    "    testy_data =[]\n",
    "    with open(trainx_file, 'r') as trainx, open(trainy_file, 'r') as trainy, open(testx_file, 'r') as testx, open(testy_file, 'r') as testy  :\n",
    "        trainy_data = extract_y(trainy)\n",
    "        testy_data = extract_y(testy)\n",
    "        \n",
    "        trainx_data = extract_data(trainx)\n",
    "        testx_data = extract_data(testx)\n",
    "\n",
    "    trainx_data = np.vstack(trainx_data)\n",
    "    testx_data = np.vstack(testx_data)\n",
    "    trainy_data = to_categorical(trainy_data)\n",
    "    testy_data = to_categorical(testy_data)\n",
    "    return trainx_data, trainy_data, testx_data, testy_data\n",
    "\n",
    "def extract_data(file):\n",
    "    data = []\n",
    "    while True:\n",
    "            try:\n",
    "                x1 = []\n",
    "                x2 = []\n",
    "                x3 = []\n",
    "                x4 = []\n",
    "                x5 = []\n",
    "                x6 = []\n",
    "                x7 = []\n",
    "                line = next(file).strip().split()\n",
    "                if len(line) >= 1680:\n",
    "                    for i in range(240):\n",
    "                        x1.append(float(line[i]))\n",
    "                        x2.append(float(line[i + 240]))\n",
    "                        x3.append(float(line[i + 480]))\n",
    "                        x4.append(float(line[i + 720]))\n",
    "                        x5.append(float(line[i + 960]))\n",
    "                        x6.append(float(line[i + 1200]))\n",
    "                        x7.append(float(line[i + 1440]))\n",
    "                else:\n",
    "                    print(\"Error train data.\")\n",
    "                x1 = array(x1)\n",
    "                x2 = array(x2)\n",
    "                x3 = array(x3)\n",
    "                x4 = array(x4)\n",
    "                x5 = array(x5)\n",
    "                x6 = array(x6)\n",
    "                x7 = array(x7)\n",
    "                line_dataset = dstack([x1, x2, x3, x4, x5, x6, x7]) \n",
    "                line_dataset = line_dataset.reshape(1,240,7)\n",
    "                data.append(line_dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def extract_y(file):\n",
    "    y = []\n",
    "    for line in file:\n",
    "        y.append(line)\n",
    "    y = array(y)\n",
    "    data = np.vstack(y)\n",
    "    data.reshape(1,len(y))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = int(depth)\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)                # (1, depth)\n",
    "    angle_rads = positions * angle_rates             # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class AddPositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, seq_len, d_model = input_shape\n",
    "        self.pos_encoding = positional_encoding(seq_len, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Ensure positional encoding has the same shape as the input\n",
    "        return inputs + self.pos_encoding[:tf.shape(inputs)[1], :tf.shape(inputs)[2]]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_enlarged_data():\n",
    "    data = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(enlarged_dataset_path):\n",
    "        filenames.append(filename.split('.')[0])\n",
    "        filepath = os.path.join(enlarged_dataset_path, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r') as file:\n",
    "                train_data = np.vstack(extract_data(file))\n",
    "                data.append(train_data)\n",
    "    return data, filenames\n",
    "\n",
    "def create_model(timesteps, features, num_classes, name = \"original_model\"):\n",
    "    # define model\n",
    "    model = Sequential(name=name)\n",
    "    model.add(LSTM(units = 128, input_shape = (timesteps, features)))\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# def create_model(timesteps, n_features, num_classes, name = \"original_model\"):\n",
    "#     inputs = Input(shape=(timesteps, n_features))\n",
    "\n",
    "#     x = Conv1D(filters=64, kernel_size=5, activation='relu')(inputs)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "#     x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "#     x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "#     x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "#     x = LSTM(units=128, return_sequences=True)(x)\n",
    "#     x = LSTM(units=128, return_sequences=True)(x)\n",
    "\n",
    "#     x = AddPositionalEncoding()(x)\n",
    "\n",
    "#     # MultiHeadAttention layer\n",
    "#     attn_output = MultiHeadAttention(num_heads=4, key_dim=128)(x, x, x)\n",
    "#     x = LayerNormalization()(attn_output + x)\n",
    "\n",
    "#     x = Dense(units=128, activation='relu')(x)\n",
    "\n",
    "#     # Global Attention layer\n",
    "#     attn = Attention()([x, x])\n",
    "#     x = LayerNormalization()(attn + x)\n",
    "\n",
    "#     # Global average pooling to reduce sequence dimension\n",
    "#     x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "#     outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "def evaluate_model(model, testX, testy,  class_names):\n",
    "    print(f\"Evaluate model {model.name}\")\n",
    "\n",
    "    y_pred = model.predict(testX)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    y_true = np.argmax(testy, axis=1)\n",
    "\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(xticks_rotation=60, ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_val_loss(models, history):\n",
    "    for model, h in zip(models, history):\n",
    "        plt.plot(h.history['val_loss'], label=model.name)\n",
    "    plt.title('model validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080000, 7)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    trainX, trainy, testX, testy = readdata(trainx_file, trainy_file, testx_file, testy_file)\n",
    "    class_names = readConfig()\n",
    "    timesteps, n_features, num_classes = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "    \n",
    "    data_augmentation, techniques = read_enlarged_data()\n",
    "    models = []\n",
    "    history = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    original_model = create_model(timesteps, n_features, num_classes)\n",
    "    original_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history.append(original_model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, validation_split = 0.2, verbose=verbose))\n",
    "    models.append(original_model)\n",
    "\n",
    "\n",
    "    for data, technique in zip(data_augmentation, techniques):\n",
    "        train_x = np.vstack([trainX, data])\n",
    "        train_y = np.vstack([trainy, trainy])\n",
    "        # train_x = scaler.fit_transform(train_x.reshape(-1, train_x.shape[-1])).reshape(train_x.shape)\n",
    "        model = create_model(timesteps, n_features, num_classes, name=technique)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        history.append(model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_split = 0.2, verbose=verbose))\n",
    "        models.append(model)\n",
    "    \n",
    "\n",
    "    all_data = np.vstack(data_augmentation)\n",
    "    all_data = np.vstack([trainX, all_data])\n",
    "    all_y = np.concatenate([trainy] * (len(data_augmentation) + 1), axis=0)\n",
    "\n",
    "    all_data_model = create_model(timesteps, n_features, num_classes, name=\"all_data_model\")\n",
    "    all_data_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    all_data_model.fit(all_data, all_y, epochs=epochs, batch_size=batch_size, validation_split = 0.2, verbose=verbose, callbacks=[early_stopping])\n",
    "\n",
    "    models.append(all_data_model)\n",
    "\n",
    "    plot_val_loss(models, history)\n",
    "    \n",
    "    for model in models:\n",
    "        model.save(f\"./model_v2.13.0/{model.name}.h5\")\n",
    "        # testX = scaler.fit_transform(testX.reshape(-1, testX.shape[-1])).reshape(testX.shape)\n",
    "        evaluate_model(model, testX, testy, class_names)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
