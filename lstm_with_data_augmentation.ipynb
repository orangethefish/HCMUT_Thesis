{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from numpy import array, dstack\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, LayerNormalization, Dense, Attention, MultiHeadAttention, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "# import pygame\n",
    "import sys\n",
    "import os\n",
    "#pygame.init()\n",
    "# engine = pyttsx3.init() # object creation\n",
    "SERIAL_PORT = '/dev/cu.DATN'\n",
    "# be sure to set this to the same rate used on the Arduino\n",
    "SERIAL_RATE = 38400\n",
    "test_counter = 240\n",
    "queue_size = 240\n",
    "verbose, epochs, batch_size = 1, 40, 300\n",
    "dataset_path = \"./datatrain_40/total/\"\n",
    "trainx_file = \"./datatrain_40/total/trainx.txt\"\n",
    "trainy_file = \"./datatrain_40/total/trainy.txt\"\n",
    "testx_file = \"./datatrain_40/total/testx.txt\"\n",
    "testy_file = \"./datatrain_40/total/testy.txt\"\n",
    "config_file = \"./datatrain_40/total/config.txt\"\n",
    "enlarged_dataset_path = \"./datatrain_40/total/augmentation/\"\n",
    "\n",
    "\n",
    "# screen_width = 800\n",
    "# screen_height = 600\n",
    "# screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "# pygame.display.set_caption(\"Real-time Display\")\n",
    "\n",
    "# Set up fonts\n",
    "#font = pygame.font.Font(None, 36)\n",
    "#WHITE = (255, 255, 255)\n",
    "\"\"\" RATE\"\"\"\n",
    "#rate = engine.getProperty('rate')   # getting details of current speaking rate\n",
    "# engine.setProperty('rate', 125)     # setting up new voice rate\n",
    "\"\"\"VOLUME\"\"\"\n",
    "#volume = engine.getProperty('volume')   #getting to know current volume level (min=0 and max=1)\n",
    "#print (volume)                          #printing current volume level\n",
    "# engine.setProperty('volume',1.0)    # setting up volume level  between 0 and 1\n",
    "\n",
    "\"\"\"VOICE\"\"\"\n",
    "#voices = engine.getProperty('voices')       #getting details of current voice\n",
    "#engine.setProperty('voice', voices[0].id)  #changing index, changes voices. o for male\n",
    "# engine.setProperty('voice', engine.getProperty('voices')[1].id)   #changing index, changes voices. 1 for female\n",
    "\n",
    "# def textToSpeech(text):\n",
    "#     engine.say(text)\n",
    "#     engine.runAndWait()\n",
    "#     engine.stop()\n",
    "    \n",
    "\n",
    "def readConfig():\n",
    "    with open('./datatrain_40/total/config.txt', 'r',encoding='utf-8') as file:\n",
    "        # Initialize an empty 2D array\n",
    "        config = []\n",
    "\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Split the line into individual words\n",
    "            words = line.strip()\n",
    "\n",
    "            # Append the words to the 2D array\n",
    "            config.append(words)\n",
    "        return config\n",
    "\n",
    "def readdata(trainx_file, trainy_file, testx_file, testy_file):\n",
    "    trainx_data =[]\n",
    "    trainy_data =[]\n",
    "    testx_data =[]\n",
    "    testy_data =[]\n",
    "    with open(trainx_file, 'r') as trainx, open(trainy_file, 'r') as trainy, open(testx_file, 'r') as testx, open(testy_file, 'r') as testy  :\n",
    "        trainy_data = extract_y(trainy)\n",
    "        testy_data = extract_y(testy)\n",
    "        \n",
    "        trainx_data = extract_data(trainx)\n",
    "        testx_data = extract_data(testx)\n",
    "\n",
    "    trainx_data = np.vstack(trainx_data)\n",
    "    testx_data = np.vstack(testx_data)\n",
    "    trainy_data = to_categorical(trainy_data)\n",
    "    testy_data = to_categorical(testy_data)\n",
    "    return trainx_data, trainy_data, testx_data, testy_data\n",
    "\n",
    "def extract_data(file):\n",
    "    data = []\n",
    "    while True:\n",
    "            try:\n",
    "                x1 = []\n",
    "                x2 = []\n",
    "                x3 = []\n",
    "                x4 = []\n",
    "                x5 = []\n",
    "                x6 = []\n",
    "                x7 = []\n",
    "                line = next(file).strip().split()\n",
    "                if len(line) >= 1680:\n",
    "                    for i in range(240):\n",
    "                        x1.append(float(line[i]))\n",
    "                        x2.append(float(line[i + 240]))\n",
    "                        x3.append(float(line[i + 480]))\n",
    "                        x4.append(float(line[i + 720]))\n",
    "                        x5.append(float(line[i + 960]))\n",
    "                        x6.append(float(line[i + 1200]))\n",
    "                        x7.append(float(line[i + 1440]))\n",
    "                else:\n",
    "                    print(\"Error train data.\")\n",
    "                x1 = array(x1)\n",
    "                x2 = array(x2)\n",
    "                x3 = array(x3)\n",
    "                x4 = array(x4)\n",
    "                x5 = array(x5)\n",
    "                x6 = array(x6)\n",
    "                x7 = array(x7)\n",
    "                line_dataset = dstack([x1, x2, x3, x4, x5, x6, x7]) \n",
    "                line_dataset = line_dataset.reshape(1,240,7)\n",
    "                data.append(line_dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def extract_y(file):\n",
    "    y = []\n",
    "    for line in file:\n",
    "        y.append(line)\n",
    "    y = array(y)\n",
    "    data = np.vstack(y)\n",
    "    data.reshape(1,len(y))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = int(depth)\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)                # (1, depth)\n",
    "    angle_rads = positions * angle_rates             # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class AddPositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, seq_len, d_model = input_shape\n",
    "        self.pos_encoding = positional_encoding(seq_len, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Ensure positional encoding has the same shape as the input\n",
    "        return inputs + self.pos_encoding[:tf.shape(inputs)[1], :tf.shape(inputs)[2]]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_enlarged_data():\n",
    "    data = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(enlarged_dataset_path):\n",
    "        filenames.append(filename.split('.')[0])\n",
    "        filepath = os.path.join(enlarged_dataset_path, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r') as file:\n",
    "                train_data = np.vstack(extract_data(file))\n",
    "                data.append(train_data)\n",
    "    return data, filenames\n",
    "\n",
    "def create_model(timesteps, features, num_classes, name = \"original_model\"):\n",
    "    # define model\n",
    "    model = Sequential(name=name)\n",
    "    model.add(LSTM(units = 128, input_shape = (timesteps, features)))\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# def create_model(timesteps, n_features, num_classes, name = \"original_model\"):\n",
    "#     inputs = Input(shape=(timesteps, n_features))\n",
    "\n",
    "#     x = Conv1D(filters=64, kernel_size=5, activation='relu')(inputs)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "#     x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "#     x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "#     x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "#     x = LSTM(units=128, return_sequences=True)(x)\n",
    "#     x = LSTM(units=128, return_sequences=True)(x)\n",
    "\n",
    "#     x = AddPositionalEncoding()(x)\n",
    "\n",
    "#     # MultiHeadAttention layer\n",
    "#     attn_output = MultiHeadAttention(num_heads=4, key_dim=128)(x, x, x)\n",
    "#     x = LayerNormalization()(attn_output + x)\n",
    "\n",
    "#     x = Dense(units=128, activation='relu')(x)\n",
    "\n",
    "#     # Global Attention layer\n",
    "#     attn = Attention()([x, x])\n",
    "#     x = LayerNormalization()(attn + x)\n",
    "\n",
    "#     # Global average pooling to reduce sequence dimension\n",
    "#     x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "#     outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "def evaluate_model(model, testX, testy,  class_names):\n",
    "    print(f\"Evaluate model {model.name}\")\n",
    "\n",
    "    y_pred = model.predict(testX)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    y_true = np.argmax(testy, axis=1)\n",
    "\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(xticks_rotation=60, ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_val_loss(models, history):\n",
    "    for model, h in zip(models, history):\n",
    "        plt.plot(h.history['val_loss'], label=model.name)\n",
    "    plt.title('model validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "12/12 [==============================] - 9s 635ms/step - loss: 1.5876 - accuracy: 0.3483 - val_loss: 2.5892 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/40\n",
      "12/12 [==============================] - 9s 752ms/step - loss: 0.8601 - accuracy: 0.7514 - val_loss: 3.4265 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "12/12 [==============================] - 9s 747ms/step - loss: 0.5247 - accuracy: 0.8489 - val_loss: 4.5952 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/40\n",
      "24/24 [==============================] - 18s 689ms/step - loss: 0.7088 - accuracy: 0.8035 - val_loss: 0.4264 - val_accuracy: 0.6933\n",
      "Epoch 2/40\n",
      "24/24 [==============================] - 16s 675ms/step - loss: 0.2400 - accuracy: 0.9232 - val_loss: 0.1972 - val_accuracy: 0.9994\n",
      "Epoch 3/40\n",
      "24/24 [==============================] - 15s 644ms/step - loss: 0.2023 - accuracy: 0.9094 - val_loss: 0.0963 - val_accuracy: 0.9739\n",
      "Epoch 4/40\n",
      "24/24 [==============================] - 16s 658ms/step - loss: 0.0469 - accuracy: 0.9917 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 5/40\n",
      "22/24 [==========================>...] - ETA: 1s - loss: 0.0170 - accuracy: 0.9980"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m         evaluate_model(model, testX, testy, class_names)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(timesteps, n_features, num_classes, name\u001b[38;5;241m=\u001b[39mtechnique)\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 25\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     26\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./old_model_weights/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    trainX, trainy, testX, testy = readdata(trainx_file, trainy_file, testx_file, testy_file)\n",
    "    class_names = readConfig()\n",
    "    timesteps, n_features, num_classes = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "    \n",
    "    data_augmentation, techniques = read_enlarged_data()\n",
    "    models = []\n",
    "    history = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    original_model = create_model(timesteps, n_features, num_classes)\n",
    "    original_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history.append(original_model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, validation_split = 0.2, verbose=verbose, callbacks=[early_stopping]))\n",
    "    models.append(original_model)\n",
    "\n",
    "\n",
    "    for data, technique in zip(data_augmentation, techniques):\n",
    "        train_x = np.vstack([trainX, data])\n",
    "        train_y = np.vstack([trainy, trainy])\n",
    "        train_x = scaler.fit_transform(train_x.reshape(-1, train_x.shape[-1])).reshape(train_x.shape)\n",
    "        model = create_model(timesteps, n_features, num_classes, name=technique)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        history.append(model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_split = 0.2, verbose=verbose, callbacks=[early_stopping]))\n",
    "        models.append(model)\n",
    "        model.save(f\"./old_model_weights/{model.name}.h5\")\n",
    "    \n",
    "\n",
    "    plot_val_loss(models, history)\n",
    "    \n",
    "    for model in models:\n",
    "        testX = scaler.fit_transform(testX.reshape(-1, testX.shape[-1])).reshape(testX.shape)\n",
    "        evaluate_model(model, testX, testy, class_names)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
