{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from numpy import array, dstack\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, LayerNormalization, Dense, Attention, MultiHeadAttention, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "# import pygame\n",
    "import sys\n",
    "import os\n",
    "#pygame.init()\n",
    "# engine = pyttsx3.init() # object creation\n",
    "SERIAL_PORT = '/dev/cu.DATN'\n",
    "# be sure to set this to the same rate used on the Arduino\n",
    "SERIAL_RATE = 38400\n",
    "test_counter = 240\n",
    "queue_size = 240\n",
    "verbose, epochs, batch_size = 1, 40, 300\n",
    "dataset_path = \"./datatrain/\"\n",
    "trainx_file = \"./datatrain/trainx.txt\"\n",
    "trainy_file = \"./datatrain/trainy.txt\"\n",
    "testx_file = \"./datatrain/testx.txt\"\n",
    "testy_file = \"./datatrain/testy.txt\"\n",
    "config_file = \"./datatrain/config.txt\"\n",
    "enlarged_dataset_path = \"./datatrain/model/\"\n",
    "\n",
    "\n",
    "# screen_width = 800\n",
    "# screen_height = 600\n",
    "# screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "# pygame.display.set_caption(\"Real-time Display\")\n",
    "\n",
    "# Set up fonts\n",
    "#font = pygame.font.Font(None, 36)\n",
    "#WHITE = (255, 255, 255)\n",
    "\"\"\" RATE\"\"\"\n",
    "#rate = engine.getProperty('rate')   # getting details of current speaking rate\n",
    "# engine.setProperty('rate', 125)     # setting up new voice rate\n",
    "\"\"\"VOLUME\"\"\"\n",
    "#volume = engine.getProperty('volume')   #getting to know current volume level (min=0 and max=1)\n",
    "#print (volume)                          #printing current volume level\n",
    "# engine.setProperty('volume',1.0)    # setting up volume level  between 0 and 1\n",
    "\n",
    "\"\"\"VOICE\"\"\"\n",
    "#voices = engine.getProperty('voices')       #getting details of current voice\n",
    "#engine.setProperty('voice', voices[0].id)  #changing index, changes voices. o for male\n",
    "# engine.setProperty('voice', engine.getProperty('voices')[1].id)   #changing index, changes voices. 1 for female\n",
    "\n",
    "# def textToSpeech(text):\n",
    "#     engine.say(text)\n",
    "#     engine.runAndWait()\n",
    "#     engine.stop()\n",
    "    \n",
    "\n",
    "def readConfig():\n",
    "    with open('./datatrain/config.txt', 'r',encoding='utf-8') as file:\n",
    "        # Initialize an empty 2D array\n",
    "        config = []\n",
    "\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Split the line into individual words\n",
    "            words = line.strip()\n",
    "\n",
    "            # Append the words to the 2D array\n",
    "            config.append(words)\n",
    "        return config\n",
    "\n",
    "def readdata(trainx_file, trainy_file, testx_file, testy_file):\n",
    "    trainx_data =[]\n",
    "    trainy_data =[]\n",
    "    testx_data =[]\n",
    "    testy_data =[]\n",
    "    with open(trainx_file, 'r') as trainx, open(trainy_file, 'r') as trainy, open(testx_file, 'r') as testx, open(testy_file, 'r') as testy  :\n",
    "        trainy_data = extract_y(trainy)\n",
    "        testy_data = extract_y(testy)\n",
    "        \n",
    "        trainx_data = extract_data(trainx)\n",
    "        testx_data = extract_data(testx)\n",
    "\n",
    "    trainx_data = np.vstack(trainx_data)\n",
    "    testx_data = np.vstack(testx_data)\n",
    "    trainy_data = to_categorical(trainy_data)\n",
    "    testy_data = to_categorical(testy_data)\n",
    "    return trainx_data, trainy_data, testx_data, testy_data\n",
    "\n",
    "def extract_data(file):\n",
    "    data = []\n",
    "    while True:\n",
    "            try:\n",
    "                x1 = []\n",
    "                x2 = []\n",
    "                x3 = []\n",
    "                x4 = []\n",
    "                x5 = []\n",
    "                x6 = []\n",
    "                x7 = []\n",
    "                line = next(file).strip().split()\n",
    "                if len(line) >= 1680:\n",
    "                    for i in range(240):\n",
    "                        x1.append(float(line[i]))\n",
    "                        x2.append(float(line[i + 240]))\n",
    "                        x3.append(float(line[i + 480]))\n",
    "                        x4.append(float(line[i + 720]))\n",
    "                        x5.append(float(line[i + 960]))\n",
    "                        x6.append(float(line[i + 1200]))\n",
    "                        x7.append(float(line[i + 1440]))\n",
    "                else:\n",
    "                    print(\"Error train data.\")\n",
    "                x1 = array(x1)\n",
    "                x2 = array(x2)\n",
    "                x3 = array(x3)\n",
    "                x4 = array(x4)\n",
    "                x5 = array(x5)\n",
    "                x6 = array(x6)\n",
    "                x7 = array(x7)\n",
    "                line_dataset = dstack([x1, x2, x3, x4, x5, x6, x7]) \n",
    "                line_dataset = line_dataset.reshape(1,240,7)\n",
    "                data.append(line_dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def extract_y(file):\n",
    "    y = []\n",
    "    for line in file:\n",
    "        y.append(line)\n",
    "    y = array(y)\n",
    "    data = np.vstack(y)\n",
    "    data.reshape(1,len(y))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = int(depth)\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)                # (1, depth)\n",
    "    angle_rads = positions * angle_rates             # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class AddPositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, seq_len, d_model = input_shape\n",
    "        self.pos_encoding = positional_encoding(seq_len, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Ensure positional encoding has the same shape as the input\n",
    "        return inputs + self.pos_encoding[:tf.shape(inputs)[1], :tf.shape(inputs)[2]]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_enlarged_data():\n",
    "    data = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(enlarged_dataset_path):\n",
    "        filenames.append(filename.split('.')[0])\n",
    "        filepath = os.path.join(enlarged_dataset_path, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r') as file:\n",
    "                train_data = np.vstack(extract_data(file))\n",
    "                data.append(train_data)\n",
    "    return data, filenames\n",
    "\n",
    "# def create_model(timesteps, features, num_classes, name = \"original_model\"):\n",
    "#     # define model\n",
    "#     model = Sequential(name=name)\n",
    "#     model.add(LSTM(units = 128, input_shape = (timesteps, features)))\n",
    "#     model.add(Dropout(0.5)) \n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(units = 64, activation='relu'))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     return model\n",
    "\n",
    "def create_model(timesteps, n_features, num_classes, name = \"original_model\"):\n",
    "    inputs = Input(shape=(timesteps, n_features))\n",
    "\n",
    "    x = Conv1D(filters=64, kernel_size=5, activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = LSTM(units=128, return_sequences=True)(x)\n",
    "    x = LSTM(units=128, return_sequences=True)(x)\n",
    "\n",
    "    x = AddPositionalEncoding()(x)\n",
    "\n",
    "    # MultiHeadAttention layer\n",
    "    attn_output = MultiHeadAttention(num_heads=4, key_dim=128)(x, x, x)\n",
    "    x = LayerNormalization()(attn_output + x)\n",
    "\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "\n",
    "    # Global Attention layer\n",
    "    attn = Attention()([x, x])\n",
    "    x = LayerNormalization()(attn + x)\n",
    "\n",
    "    # Global average pooling to reduce sequence dimension\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, testX, testy,  class_names):\n",
    "    print(f\"Evaluate model {model.name}\")\n",
    "\n",
    "    y_pred = model.predict(testX)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    y_true = np.argmax(testy, axis=1)\n",
    "\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(xticks_rotation=60, ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_val_loss(models, history):\n",
    "    for model, h in zip(models, history):\n",
    "        plt.plot(h.history['val_loss'], label=model.name)\n",
    "    plt.title('model validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2/2 [==============================] - 7s 1s/step - loss: 2.9697 - accuracy: 0.1224 - val_loss: 3.0030 - val_accuracy: 0.0714\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 2.3358 - accuracy: 0.2806 - val_loss: 3.0338 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 2.0598 - accuracy: 0.3495 - val_loss: 2.9124 - val_accuracy: 0.0816\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 1.7309 - accuracy: 0.4796 - val_loss: 2.5119 - val_accuracy: 0.2143\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 1.3972 - accuracy: 0.5842 - val_loss: 2.0960 - val_accuracy: 0.3980\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 1.1904 - accuracy: 0.6276 - val_loss: 2.2013 - val_accuracy: 0.3776\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 1.0197 - accuracy: 0.6658 - val_loss: 2.0414 - val_accuracy: 0.4184\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.9179 - accuracy: 0.6913 - val_loss: 1.3317 - val_accuracy: 0.4796\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.7809 - accuracy: 0.7321 - val_loss: 1.2719 - val_accuracy: 0.5408\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.7385 - accuracy: 0.7092 - val_loss: 0.8912 - val_accuracy: 0.6531\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.6672 - accuracy: 0.7908 - val_loss: 1.2169 - val_accuracy: 0.6327\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.5690 - accuracy: 0.8163 - val_loss: 1.4920 - val_accuracy: 0.4082\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' dense_3/kernel:0': Shape mismatch.The variable shape (128, 15), and the assigned value shape (128, 40) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m         evaluate_model(model, testX, testy, class_names)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m train_x \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(train_x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, train_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mreshape(train_x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(timesteps, n_features, num_classes, name\u001b[38;5;241m=\u001b[39mtechnique)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model/SADeepConvLSTM_TransferLearning.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mfit(train_x, train_y, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39mverbose, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:4361\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[1;34m(variable, value)\u001b[0m\n\u001b[0;32m   4358\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[0;32m   4359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4360\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[1;32m-> 4361\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot assign value to variable ' dense_3/kernel:0': Shape mismatch.The variable shape (128, 15), and the assigned value shape (128, 40) are incompatible."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    trainX, trainy, testX, testy = readdata(trainx_file, trainy_file, testx_file, testy_file)\n",
    "    class_names = readConfig()\n",
    "    timesteps, n_features, num_classes = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "    \n",
    "    data_augmentation, techniques = read_enlarged_data()\n",
    "    models = []\n",
    "    history = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    original_model = create_model(timesteps, n_features, num_classes)\n",
    "    original_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history.append(original_model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, validation_split = 0.2, verbose=verbose, callbacks=[early_stopping]))\n",
    "    models.append(original_model)\n",
    "\n",
    "\n",
    "    for data, technique in zip(data_augmentation, techniques):\n",
    "        train_x = np.vstack([trainX, data])\n",
    "        train_y = np.vstack([trainy, trainy])\n",
    "        train_x = scaler.fit_transform(train_x.reshape(-1, train_x.shape[-1])).reshape(train_x.shape)\n",
    "        model = create_model(timesteps, n_features, num_classes, name=technique)\n",
    "        # model.load_weights(f\"./model/SADeepConvLSTM_TransferLearning.h5\")\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        history.append(model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_split = 0.2, verbose=verbose, callbacks=[early_stopping]))\n",
    "        models.append(model)\n",
    "        model.save(f\"./model/{model.name}_SADeepConvLSTM.h5\")\n",
    "    \n",
    "\n",
    "    plot_val_loss(models, history)\n",
    "    \n",
    "    for model in models:\n",
    "        evaluate_model(model, testX, testy, class_names)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
