{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dstack\n",
    "from numpy import array, dstack\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx_file = \"./datatrain_40/total/trainx.txt\"\n",
    "trainy_file = \"./datatrain_40/total/trainy.txt\"\n",
    "testx_file = \"./datatrain_40/total/testx.txt\"\n",
    "testy_file = \"./datatrain_40/total/testy.txt\"\n",
    "config_file = \"./datatrain_40/total/config.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_y(file):\n",
    "    y = []\n",
    "    for line in file:\n",
    "        y.append(line)\n",
    "    y = array(y)\n",
    "    data = np.vstack(y)\n",
    "    data.reshape(1,len(y))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file):\n",
    "    data = []\n",
    "    while True:\n",
    "            try:\n",
    "                x1 = []\n",
    "                x2 = []\n",
    "                x3 = []\n",
    "                x4 = []\n",
    "                x5 = []\n",
    "                x6 = []\n",
    "                x7 = []\n",
    "                line = next(file).strip().split()\n",
    "                if len(line) >= 1680:\n",
    "                    for i in range(240):\n",
    "                        x1.append(float(line[i]))\n",
    "                        x2.append(float(line[i + 240]))\n",
    "                        x3.append(float(line[i + 480]))\n",
    "                        x4.append(float(line[i + 720]))\n",
    "                        x5.append(float(line[i + 960]))\n",
    "                        x6.append(float(line[i + 1200]))\n",
    "                        x7.append(float(line[i + 1440]))\n",
    "                else:\n",
    "                    print(\"Error train data.\")\n",
    "                x1 = array(x1)\n",
    "                x2 = array(x2)\n",
    "                x3 = array(x3)\n",
    "                x4 = array(x4)\n",
    "                x5 = array(x5)\n",
    "                x6 = array(x6)\n",
    "                x7 = array(x7)\n",
    "                line_dataset = dstack([x1, x2, x3, x4, x5, x6, x7]) \n",
    "                line_dataset = line_dataset.reshape(1,240,7)\n",
    "                data.append(line_dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(trainx_file, trainy_file, testx_file, testy_file):\n",
    "    trainx_data =[]\n",
    "    trainy_data =[]\n",
    "    testx_data =[]\n",
    "    testy_data =[]\n",
    "    with open(trainx_file, 'r') as trainx, open(trainy_file, 'r') as trainy, open(testx_file, 'r') as testx, open(testy_file, 'r') as testy  :\n",
    "        trainy_data = extract_y(trainy)\n",
    "        testy_data = extract_y(testy)\n",
    "        \n",
    "        trainx_data = extract_data(trainx)\n",
    "        testx_data = extract_data(testx)\n",
    "\n",
    "    trainx_data = np.vstack(trainx_data)\n",
    "    testx_data = np.vstack(testx_data)\n",
    "    trainy_data = to_categorical(trainy_data)\n",
    "    testy_data = to_categorical(testy_data)\n",
    "    return trainx_data, trainy_data, testx_data, testy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_synthetic_quality(original_df, synthetic_df, max_lag=10):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of synthetic time series data compared to original data.\n",
    "    \n",
    "    Parameters:\n",
    "    original_df (pd.DataFrame): Original time series data\n",
    "    synthetic_df (pd.DataFrame): Synthetic time series data\n",
    "    max_lag (int): Maximum lag for autocorrelation calculation (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing various evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Input validation\n",
    "    if original_df.shape != synthetic_df.shape:\n",
    "        raise ValueError(\"Original and synthetic dataframes must have the same shape\")\n",
    "    \n",
    "    # Basic Statistical Metrics\n",
    "    def calculate_statistical_metrics(orig_data, synt_data):\n",
    "        try:\n",
    "            # Convert to numpy arrays for faster computation\n",
    "            orig_array = np.array(orig_data)\n",
    "            synt_array = np.array(synt_data)\n",
    "            \n",
    "            return {\n",
    "                'mean_difference': abs(np.mean(orig_array) - np.mean(synt_array)),\n",
    "                'std_difference': abs(np.std(orig_array) - np.std(synt_array)),\n",
    "                'ks_test': stats.ks_2samp(orig_array, synt_array).statistic,\n",
    "                # Only sort once for Wasserstein distance\n",
    "                'wasserstein': stats.wasserstein_distance(\n",
    "                    np.sort(orig_array), \n",
    "                    np.sort(synt_array)\n",
    "                )\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in statistical metrics calculation: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # Time Series Specific Metrics\n",
    "    def calculate_ts_metrics(orig_data, synt_data):\n",
    "        try:\n",
    "            return {\n",
    "                'mse': mean_squared_error(orig_data, synt_data),\n",
    "                'rmse': np.sqrt(mean_squared_error(orig_data, synt_data)),\n",
    "                'mae': mean_absolute_error(orig_data, synt_data)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in time series metrics calculation: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # Correlation Structure Preservation\n",
    "    def calculate_correlation_preservation():\n",
    "        try:\n",
    "            # Calculate correlations once\n",
    "            orig_corr = original_df.corr().fillna(0)\n",
    "            synt_corr = synthetic_df.corr().fillna(0)\n",
    "            return np.mean(np.abs(orig_corr - synt_corr))\n",
    "        except Exception as e:\n",
    "            print(f\"Error in correlation preservation calculation: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # Autocorrelation Preservation with timeout\n",
    "    def calculate_autocorr_preservation(orig_data, synt_data):\n",
    "        try:\n",
    "            # Pre-calculate autocorrelations\n",
    "            orig_autocorr = []\n",
    "            synt_autocorr = []\n",
    "            \n",
    "            for i in range(1, min(max_lag + 1, len(orig_data))):\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    orig_ac = pd.Series(orig_data).autocorr(lag=i)\n",
    "                    synt_ac = pd.Series(synt_data).autocorr(lag=i)\n",
    "                    \n",
    "                    # Handle NaN values\n",
    "                    if pd.isna(orig_ac) or pd.isna(synt_ac):\n",
    "                        continue\n",
    "                        \n",
    "                    orig_autocorr.append(orig_ac)\n",
    "                    synt_autocorr.append(synt_ac)\n",
    "            \n",
    "            if not orig_autocorr or not synt_autocorr:\n",
    "                return 0\n",
    "                \n",
    "            return np.mean(np.abs(np.array(orig_autocorr) - np.array(synt_autocorr)))\n",
    "        except Exception as e:\n",
    "            print(f\"Error in autocorrelation preservation calculation: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # Calculate metrics for each column with progress bar\n",
    "    print(\"Calculating metrics for each column...\")\n",
    "    for column in tqdm(original_df.columns):\n",
    "        orig_data = original_df[column].values\n",
    "        synt_data = synthetic_df[column].values\n",
    "        \n",
    "        metrics[column] = {\n",
    "            'statistical_metrics': calculate_statistical_metrics(orig_data, synt_data),\n",
    "            'timeseries_metrics': calculate_ts_metrics(orig_data, synt_data),\n",
    "            'autocorr_preservation': calculate_autocorr_preservation(orig_data, synt_data)\n",
    "        }\n",
    "    \n",
    "    # Overall dataset metrics\n",
    "    metrics['overall'] = {\n",
    "        'correlation_preservation': calculate_correlation_preservation(),\n",
    "        'shape_match': original_df.shape == synthetic_df.shape\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_evaluation_summary(metrics):\n",
    "    \"\"\"\n",
    "    Print a formatted summary of the evaluation metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    metrics (dict): Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Synthetic Data Quality Evaluation Summary ===\\n\")\n",
    "    \n",
    "    # Print overall metrics\n",
    "    print(\"Overall Dataset Metrics:\")\n",
    "    print(f\"Shape Match: {metrics['overall']['shape_match']}\")\n",
    "    if metrics['overall']['correlation_preservation'] is not None:\n",
    "        print(f\"Correlation Structure Preservation Score: {metrics['overall']['correlation_preservation']:.4f}\")\n",
    "    print(\"\\nDetailed Metrics by Column:\")\n",
    "    \n",
    "    # Print column-specific metrics\n",
    "    for column in metrics.keys():\n",
    "        if column != 'overall':\n",
    "            print(f\"\\n{column}:\")\n",
    "            stats = metrics[column]['statistical_metrics']\n",
    "            ts = metrics[column]['timeseries_metrics']\n",
    "            \n",
    "            if stats:\n",
    "                print(\"Statistical Metrics:\")\n",
    "                print(f\"  Mean Difference: {stats['mean_difference']:.4f}\")\n",
    "                print(f\"  Std Difference: {stats['std_difference']:.4f}\")\n",
    "                print(f\"  KS Test Statistic: {stats['ks_test']:.4f}\")\n",
    "                print(f\"  Wasserstein Distance: {stats['wasserstein']:.4f}\")\n",
    "            \n",
    "            if ts:\n",
    "                print(\"Time Series Metrics:\")\n",
    "                print(f\"  RMSE: {ts['rmse']:.4f}\")\n",
    "                print(f\"  MAE: {ts['mae']:.4f}\")\n",
    "            \n",
    "            if metrics[column]['autocorr_preservation'] is not None:\n",
    "                print(f\"Autocorrelation Preservation Score: {metrics[column]['autocorr_preservation']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy, testX, testy = readdata(trainx_file, trainy_file, testx_file, testy_file)\n",
    "trainy = np.argmax(trainy, axis=1)\n",
    "class_labels = np.unique(trainy)\n",
    "# original_data = []\n",
    "# for label in class_labels:\n",
    "#     reshaped_data = trainX[trainy == label].reshape(-1 ,7)\n",
    "#     df = pd.DataFrame(reshaped_data)\n",
    "#     original_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"datasource\"\n",
    "original_data = []\n",
    "for i in range(len(class_labels)):\n",
    "    try:\n",
    "        data = pd.read_csv(f\"{data_path}/{i}.csv\")\n",
    "        original_data.append(data)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Original and synthetic dataframes must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m synthetic_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msynthetic_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/label_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(original_data[i].shape, synthetic_data.shape)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print(original_data[i].head())\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(synthetic_data.head())  \u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_synthetic_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m print_evaluation_summary(metrics)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[77], line 17\u001b[0m, in \u001b[0;36mevaluate_synthetic_quality\u001b[1;34m(original_df, synthetic_df, max_lag)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Input validation\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_df\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m synthetic_df\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal and synthetic dataframes must have the same shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Basic Statistical Metrics\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_statistical_metrics\u001b[39m(orig_data, synt_data):\n",
      "\u001b[1;31mValueError\u001b[0m: Original and synthetic dataframes must have the same shape"
     ]
    }
   ],
   "source": [
    "synthetic_path = \"synthetic_data\"\n",
    "\n",
    "for i in range(len(class_labels)):\n",
    "    try:\n",
    "        synthetic_data = pd.read_csv(f\"{synthetic_path}/label_{i}.csv\")\n",
    "        # print(original_data[i].shape, synthetic_data.shape)\n",
    "        # print(original_data[i].head())\n",
    "        # print(synthetic_data.head())  \n",
    "        metrics = evaluate_synthetic_quality(original_data[i], synthetic_data)\n",
    "        print_evaluation_summary(metrics)\n",
    "        print(\"\\n\\n\")\n",
    "        break\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
